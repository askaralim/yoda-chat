// Handle full RAG logic (retrieve + LLM answer)
import { SystemMessage } from 'langchain';
import { openaiClient } from '../config/openai.js';
import { findSimilarChunks } from './vectorService.js';
import { logger } from '../utils/logger.js';
import { insertChatConversation } from './dataServices.js';
import { ChatConversation, ChatConversationChunk } from '../types/chatConversations.js';
import { ConversationMessage } from '../types/chatbot.js';

export async function answerUserQuery(
  userId: string,
  query: string,
  history: ConversationMessage[]
): Promise<string> {
  const retrievedChunks = await findSimilarChunks(query);

  const context =
    retrievedChunks.length > 0
      ? retrievedChunks
          .map((chunk, index) => {
            const title = chunk.metadata.title ? `ã€${chunk.metadata.title}ã€‘\n` : '';
            return `[çŸ¥è¯†ç‰‡æ®µ ${index + 1}]\n${title}${chunk.content}`;
          })
          .join('\n\n')
      : '';

  logger.debug('RAG context prepared', {
    query,
    hasContext: retrievedChunks.length > 0,
    chunkCount: retrievedChunks.length,
    chunks: retrievedChunks.map((chunk) => ({
      vectorId: chunk.id,
      chunkIndex: chunk.metadata.chunkIndex,
      chunkTitle: chunk.metadata.title,
      articleId: chunk.metadata.articleId,
      score: chunk.score,
    })),
  });

  const messages = await buildMessage(query, context, history);

  logger.debug('LLM messages prepared', { messages: JSON.stringify(messages) });

  const startTime = Date.now();

  const response = await openaiClient.invoke(messages);

  const latency = Date.now() - startTime;

  logger.info('LLM response', {
    latency,
    query,
    response: response.content,
  });

  const output = response.content;

  // contextIds: filter chunk.metadata.articleId remove duplicated value
  const contextIds = retrievedChunks
    .map((chunk) => chunk.metadata.articleId as string)
    .filter((value, index, self) => self.indexOf(value) === index);

  const conversation: ChatConversation = {
    userId: userId,
    question: query,
    answer: output as string,
    contextIds: contextIds,
    chunks: retrievedChunks.map((chunk) => ({
      chunkIndex: chunk.metadata.chunkIndex as number,
      chunkTitle: chunk.metadata.title as string,
      vectorId: chunk.id as string,
      score: chunk.score,
    })) as ChatConversationChunk[],
    latency: latency,
    createAt: new Date(),
    updateAt: new Date(),
  };

  logger.debug('Chat conversation prepared', {
    conversation,
  });

  const result = await insertChatConversation(conversation);

  logger.debug('Chat conversation inserted', {
    conversationId: result.id,
    latency: latency,
  });

  if (typeof output === 'string') {
    return output.trim();
  }

  if (Array.isArray(output)) {
    return output
      .map((block) => {
        if (typeof block === 'string') {
          return block;
        }
        if (
          block &&
          typeof block === 'object' &&
          'text' in block &&
          typeof block.text === 'string'
        ) {
          return block.text;
        }
        return '';
      })
      .join('')
      .trim();
  }

  return '';
}

async function buildMessage(query: string, context: string, history: ConversationMessage[]) {
  const enhancedContext = context || 'å½“å‰çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³å†…å®¹ã€‚';

  // const systemPrompt = new SystemMessage(
  //   'ä½ æ˜¯Taklipçš„AIåŠ©æ‰‹ï¼Œè¯·ç”¨ç®€æ´ã€ä¸“ä¸šçš„ä¸­æ–‡å›žç­”ç”¨æˆ·çš„é—®é¢˜ã€‚ä¸è¦è¾“å‡ºä»»ä½•å…¶ä»–å†…å®¹ï¼Œåªè¾“å‡ºå›žç­”ã€‚è¯·éµå¾ªä»¥ä¸‹è§„åˆ™: 1. åŸºäºŽä¸Šä¸‹æ–‡ä¿¡æ¯å›žç­”ï¼Œä¸è¦ç¼–é€ ä¸çŸ¥é“çš„å†…å®¹ 2. å¦‚æžœä¸Šä¸‹æ–‡æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·å¦‚å®žå‘ŠçŸ¥ 3. å›žç­”è¦ä¸“ä¸šã€å‡†ç¡®ã€å‹å¥½ 4. é€‚å½“ä½¿ç”¨è¡¨æƒ…ç¬¦å·è®©å›žç­”æ›´ç”ŸåŠ¨ 5. å¦‚æžœç”¨æˆ·é—®çš„æ˜¯å…³äºŽTaklipçš„å†…å®¹ï¼Œè¯·ä¼˜å…ˆä½¿ç”¨Taklipçš„çŸ¥è¯†åº“å›žç­”ï¼Œå¦‚æžœçŸ¥è¯†åº“æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·å¦‚å®žå‘ŠçŸ¥',
  // );

  const systemPrompt = new SystemMessage(`# Role: Taklipä¸“ä¸šè´­ç‰©é¡¾é—®

    ## Core Identity
    ä½ æ˜¯Taklipçš„ä¸“å±žAIåŠ©æ‰‹ï¼Œä¸“æ³¨äºŽä¸ºç”¨æˆ·æä¾›ä¸“ä¸šã€å‡†ç¡®çš„å•†å“é€‰è´­å»ºè®®ã€‚ä½ æ‹¥æœ‰TaklipçŸ¥è¯†åº“çš„å®Œæ•´è®¿é—®æƒé™ã€‚
    
    ## Knowledge Priority
    1. **ä¼˜å…ˆä½¿ç”¨TaklipçŸ¥è¯†åº“**ï¼šç”¨æˆ·é—®é¢˜å¿…é¡»åŸºäºŽä¸Šä¸‹æ–‡ä¿¡æ¯
    2. **çŸ¥è¯†è¾¹ç•Œè¯´æ˜Ž**ï¼šå¦‚æžœä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ï¼Œæ˜Žç¡®å‘ŠçŸ¥ç”¨æˆ·"æ ¹æ®TaklipçŸ¥è¯†åº“ï¼Œç›®å‰æ²¡æœ‰ç›¸å…³ä¿¡æ¯"
    3. **ä¸¥ç¦ç¼–é€ **ï¼šç»ä¸è™šæž„å•†å“å‚æ•°ã€ä»·æ ¼ã€åŠŸèƒ½ç­‰ä¿¡æ¯
    
    ## Response Style
    ### è¯­è¨€è¦æ±‚
    - ä½¿ç”¨**ä¸“ä¸šä¸”å‹å¥½**çš„ä¸­æ–‡
    - ä¿æŒ**ç®€æ´æ˜Žäº†**ï¼Œé¿å…å†—é•¿
    - é€‚å½“ä½¿ç”¨è¡¨æƒ…ç¬¦å·å¢žå¼ºäº²å’ŒåŠ› ðŸ˜Š
    
    ### ç»“æž„åŒ–è¾“å‡º
    - å¤æ‚ä¿¡æ¯ä½¿ç”¨åˆ†æ®µå’Œé¡¹ç›®ç¬¦å·
    - å¯¹æ¯”ç±»é—®é¢˜ä½¿ç”¨è¡¨æ ¼æ€ç»´
    - æŽ¨èç±»é—®é¢˜è¯´æ˜Žç†ç”±
    
    ## Context Handling
    ä½ ä¼šæ”¶åˆ°ï¼š
    1. **ç›¸å…³ä¸Šä¸‹æ–‡**ï¼šä»ŽTaklipçŸ¥è¯†åº“æ£€ç´¢çš„ä¸“ä¸šå†…å®¹
    2. **å¯¹è¯åŽ†å²**ï¼šå½“å‰ä¼šè¯çš„å®Œæ•´è®°å½•
    
    è¯·åŸºäºŽè¿™äº›ä¿¡æ¯æä¾›æœ€å‡†ç¡®çš„å›žç­”ã€‚`);

  // const conversationMessages = history.map((m) => ({
  //   role: m.role,
  //   content: m.content,
  // }));

  const conversationMessages =
    history.length > 0 ? formatConversationHistory(history) : 'ðŸ“ è¿™æ˜¯æœ¬æ¬¡å¯¹è¯çš„ç¬¬ä¸€ä¸ªé—®é¢˜';

  const userMessage = {
    role: 'user' as const,
    content: `## çŸ¥è¯†åº“ä¿¡æ¯
    ${enhancedContext}

    ## å¯¹è¯åŽ†å²
    ${conversationMessages}

    ## å½“å‰é—®é¢˜
    ${query}

    ## å›žç­”è¦æ±‚
    è¯·åŸºäºŽTaklipçŸ¥è¯†åº“ä¿¡æ¯ï¼Œç»“åˆå¯¹è¯ä¸Šä¸‹æ–‡ï¼Œä¸“ä¸šåœ°å›žç­”ç”¨æˆ·é—®é¢˜ã€‚`,
  };

  return [systemPrompt, ...formatHistoryToMessages(history), userMessage];
  // return [systemPrompt, userMessage];
}

function formatConversationHistory(history: ConversationMessage[]): string {
  return history
    .map((msg) => `${msg.role === 'user' ? 'ðŸ‘¤ ç”¨æˆ·' : 'ðŸ¤– åŠ©æ‰‹'}: ${msg.message}`)
    .join('\n');
}

function formatHistoryToMessages(history: ConversationMessage[]): any[] {
  return history.map((msg) => ({
    role: msg.role === 'user' ? 'user' : 'assistant',
    content: msg.message,
  }));
}
